{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uPYUt0UkFyE5"
      },
      "outputs": [],
      "source": [
        "!pip install google-generativeai langchain-google-genai streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o073J4JQH2qf"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyBsc_A-lJEQrSOasBtwsYVcFkXwwzDfq7Y\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "uBOc7G6Zpucc"
      },
      "outputs": [],
      "source": [
        "!pip install -q streamlit\n",
        "\n",
        "!npm install localtunnel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c0x6GRvksSxt"
      },
      "outputs": [],
      "source": [
        "import urllib\n",
        "print(\"Password/Enpoint IP for localtunnel is:\",urllib.request.urlopen('https://ipv4.icanhazip.com').read().decode('utf8').strip(\"\\n\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RVcptPckilR_"
      },
      "outputs": [],
      "source": [
        "%%writefile app.py\n",
        "\n",
        "import streamlit as st\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "\n",
        "\n",
        "st.title(\"Khannan Vakeel- Mining Law Assistant\")\n",
        "\n",
        "\n",
        "# Set Google API key\n",
        "os.environ['GOOGLE_API_KEY'] = \"AIzaSyBsc_A-lJEQrSOasBtwsYVcFkXwwzDfq7Y\"\n",
        "genai.configure(api_key = os.environ['GOOGLE_API_KEY'])\n",
        "\n",
        "\n",
        "# Create the Model\n",
        "model = genai.GenerativeModel('gemini-pro')\n",
        "\n",
        "if 'messages' not in st.session_state:\n",
        "    st.session_state['messages'] = []\n",
        "\n",
        "# Initialize chat history\n",
        "if \"messages\" not in st.session_state:\n",
        "    st.session_state.messages = [\n",
        "        {\n",
        "            \"role\":\"assistant\",\n",
        "            \"content\":\"Ask me Anything\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "\n",
        "# Display chat messages from history on app rerun\n",
        "for message in st.session_state.messages:\n",
        "    with st.chat_message(message[\"role\"]):\n",
        "        st.markdown(message[\"content\"])\n",
        "\n",
        "# Process and store Query and Response\n",
        "def llm_function(query):\n",
        "    response = model.generate_content(query)\n",
        "\n",
        "\n",
        "    # Displaying the Assistant Message\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        st.markdown(response.text)\n",
        "\n",
        "\n",
        "    # Storing the User Message\n",
        "    st.session_state.messages.append(\n",
        "        {\n",
        "            \"role\":\"user\",\n",
        "            \"content\": query\n",
        "        }\n",
        "    )\n",
        "\n",
        " # Storing the User Message\n",
        "    st.session_state.messages.append(\n",
        "        {\n",
        "            \"role\":\"assistant\",\n",
        "            \"content\": response.text\n",
        "        }\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "# Accept user input\n",
        "query = st.chat_input(\"What is up?\")\n",
        "\n",
        "# Calling the Function when Input is Provided\n",
        "if query:\n",
        "    # Displaying the User Message\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.markdown(query)\n",
        "\n",
        "\n",
        "    llm_function(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnZuyjNQzj-x"
      },
      "outputs": [],
      "source": [
        "\"\"\"# Create a new store for housing your documents.\n",
        "corpus_store = GoogleVectorStore.create_corpus(display_name=\"My Corpus\")\n",
        "\n",
        "# Create a new document under the above corpus.\n",
        "document_store = GoogleVectorStore.create_document(\n",
        "    corpus_id=corpus_store.corpus_id, display_name=\"My Document\"\n",
        ")\n",
        "\n",
        "# Upload some texts to the document.\n",
        "text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=0)\n",
        "for file in DirectoryLoader(path=\"/content/theminesact1952.pdf\").load():\n",
        "    documents = text_splitter.split_documents([file])\n",
        "    document_store.add_documents(documents)\n",
        "\n",
        "# Talk to your entire corpus with possibly many documents.\n",
        "aqa = corpus_store.as_aqa()\n",
        "answer = aqa.invoke(\"What is the maximum hours of work\")\n",
        "\n",
        "# Read the response along with the attributed passages and answerability.\n",
        "print(response.answer)\n",
        "print(response.attributed_passages)\n",
        "print(response.answerable_probability)\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yUy2UU0ppqWj"
      },
      "outputs": [],
      "source": [
        "!streamlit run app.py &>/content/logs.txt & curl https://loca.lt/mytunnelpassword"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEpHZLmcqeOg",
        "outputId": "2e035f8f-36f9-4b73-dafd-6576413da666"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[K\u001b[?25hnpx: installed 22 in 1.785s\n",
            "your url is: https://thirty-moons-sneeze.loca.lt\n"
          ]
        }
      ],
      "source": [
        "!npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}